{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmtu14/BookP/blob/master/gemma2_smallcv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "36OCGfNx76YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6287681b-3a53-4b15-cb3d-d9f8366a38f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets\n",
        "# !pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_DZWE2EV2Q6",
        "outputId": "32f7caad-fa83-4aa1-b349-7198c40a29a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wsdm\n"
          ]
        }
      ],
      "source": [
        "%cd wsdm\n",
        "%load_ext commonlib.utils\n",
        "from commonlib.utils import initialize_experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iERxuo5GWDqW",
        "outputId": "756a34c7-09d9-4330-b0a6-4eab76d27a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-16, 11:24:56 - The default behavior of function 'print' has been overrided.\n",
            "2024-12-16, 11:24:56 - Set EXPERIMENT_NAME to gemma2_smallcv\n",
            "2024-12-16, 11:24:56 - Set PATH.COMMON to /content/wsdm\n",
            "2024-12-16, 11:24:56 - Set PATH.DRIVE to /content/drive/MyDrive/kaggle/wsdm/gemma2_smallcv\n",
            "2024-12-16, 11:24:56 - Set PATH.DATA to /content/drive/MyDrive/kaggle/wsdm/data\n",
            "2024-12-16, 11:24:56 - Set PATH.LOGS to /content/drive/MyDrive/kaggle/wsdm/logs/gemma2_smallcv\n",
            "2024-12-16, 11:24:56 - Set PATH.OOF to /content/drive/MyDrive/kaggle/wsdm/oof/gemma2_smallcv\n",
            "2024-12-16, 11:24:56 - Set PATH.MODELS to /content/drive/MyDrive/kaggle/wsdm/models/gemma2_smallcv\n",
            "2024-12-16, 11:24:56 - Set PATH.BACKBONE_MODELS to /content/drive/MyDrive/kaggle/wsdm/backbone_models\n",
            "/content/wsdm\n"
          ]
        }
      ],
      "source": [
        "initialize_experiment(\n",
        "    name=\"gemma2_smallcv\",\n",
        "    version=\"\",\n",
        "    gpus=[0],\n",
        ")\n",
        "# Import from commonlib\n",
        "from commonlib.header import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eDBJtMP32NGh"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Gemma2ForSequenceClassification,\n",
        "    GemmaTokenizerFast,\n",
        "    Gemma2Config,\n",
        "    MT5ForSequenceClassification,\n",
        "    MT5TokenizerFast,\n",
        "    MT5Config,\n",
        "    Qwen2ForSequenceClassification,\n",
        "    Qwen2TokenizerFast,\n",
        "    Qwen2Config,\n",
        "    PreTrainedTokenizerBase,\n",
        "    EvalPrediction,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback,\n",
        "    set_seed\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "import bitsandbytes\n",
        "from dataclasses import dataclass\n",
        "# Miscellaneous\n",
        "import warnings\n",
        "import keras\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Q3RQe-rCdUw"
      },
      "outputs": [],
      "source": [
        "def set_all_seeds(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    set_seed(seed)\n",
        "    # For TensorFlow (if used)\n",
        "    # try:\n",
        "    #     import tensorflow as tf\n",
        "    #     tf.random.set_seed(seed)\n",
        "    #     tf.config.experimental.enable_op_determinism()\n",
        "    # except ImportError:\n",
        "    #     pass\n",
        "    # Ensure deterministic behavior in PyTorch\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_all_seeds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFY3A06U3LkQ",
        "outputId": "8edd85d4-e422-4d0d-c587-7b66aadf66f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-16, 11:25:14 - 3.5.0\n",
            "2024-12-16, 11:25:14 - 4.46.3\n",
            "2024-12-16, 11:25:14 - 2.5.1+cu121\n",
            "2024-12-16, 11:25:14 - 12.1\n"
          ]
        }
      ],
      "source": [
        "# print(tf.__version__)\n",
        "print(keras.__version__)\n",
        "print(transformers.__version__)\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nbr4fR99ibTg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K6qU35pm3Qoo"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    output_dir: str = PATH.MODELS\n",
        "    checkpoint: str = os.path.join(\n",
        "        PATH.BACKBONE_MODELS,\n",
        "        \"models--unsloth--gemma-2-9b-it-bnb-4bit\",\n",
        "        \"snapshots\",\n",
        "        \"27b027bcbb6b1861b02551d5c699d5d07f29610a\"\n",
        "    )  # 4-bit quantized gemma-2-9b-instruct\n",
        "    # checkpoint: str = \"Qwen/Qwen2-0.5B\"\n",
        "    max_length: int = 1024\n",
        "    n_splits: int = 5\n",
        "    fold_idx: int = 0\n",
        "    optim_type: str = \"adamw_8bit\"\n",
        "    per_device_train_batch_size: int = 24\n",
        "    gradient_accumulation_steps: int = 1  # global batch size is 8\n",
        "    per_device_eval_batch_size: int = 24\n",
        "    n_epochs: int = 1\n",
        "    # freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
        "    freeze_layers: int = 0\n",
        "    lr: float = 1e-4\n",
        "    warmup_steps: int = 120\n",
        "    lora_r: int = 16\n",
        "    lora_alpha: float = lora_r * 2\n",
        "    lora_dropout: float = 0.05\n",
        "    lora_bias: str = \"none\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Automatically set per_device_eval_batch_size to 2 * per_device_train_batch_size if not specified\n",
        "        if self.per_device_eval_batch_size is None:\n",
        "            self.per_device_eval_batch_size = 2 * self.per_device_train_batch_size\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kCS3AhmZ3zOF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6yNYkIGEPPk"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1vsT_wwzBOpL"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    # only target self-attention\n",
        "    target_modules=[\n",
        "        # \"q\", \"k\", \"v\", \"o\",\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        # \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    # layers_to_transform=[i for i in range(24) if i >= config.freeze_layers],\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    bias=config.lora_bias,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8Tzjpc3_rYSO"
      },
      "outputs": [],
      "source": [
        "# tokenizer = Qwen2TokenizerFast.from_pretrained(config.checkpoint)\n",
        "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\n",
        "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Un1Cw0TwZGg",
        "outputId": "35f5ce1a-e2c0-4d10-fa9a-d5f3f33b8872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/kaggle/wsdm/backbone_models/models--unsloth--gemma-2-9b-it-bnb-4bit/snapshots/27b027bcbb6b1861b02551d5c699d5d07f29610a and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.95 s, sys: 3.06 s, total: 10 s\n",
            "Wall time: 1min 1s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma2ForSequenceClassification(\n",
              "      (model): Gemma2Model(\n",
              "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-41): 42 x Gemma2DecoderLayer(\n",
              "            (self_attn): Gemma2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): Gemma2RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Gemma2MLP(\n",
              "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=3584, out_features=2, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=3584, out_features=2, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "%%time\n",
        "# model = Qwen2ForSequenceClassification.from_pretrained(\n",
        "#     config.checkpoint,\n",
        "#     num_labels=2,\n",
        "#     torch_dtype=torch.float16,\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "model = Gemma2ForSequenceClassification.from_pretrained(\n",
        "    config.checkpoint,\n",
        "    num_labels=2,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "# model.config.pad_token_id = model.config.eos_token_id\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdCVCvox9a-M",
        "outputId": "a490ccfc-2827-49dc-b184-03493049216a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 17,898,496 || all params: 9,259,611,648 || trainable%: 0.1933\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q2DBPXeZ35WN"
      },
      "outputs": [],
      "source": [
        "# (score): ModulesToSaveWrapper(\n",
        "#         (original_module): Linear(in_features=3584, out_features=2, bias=False)\n",
        "#         (modules_to_save): ModuleDict(\n",
        "#           (default): Linear(in_features=3584, out_features=2, bias=False)\n",
        "#         )\n",
        "#       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4dSxZBuE4HP6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2A8Xx-ERWw"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kvC4a4Bn-zOJ"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(os.path.join(PATH.DATA, \"train.csv\"))\n",
        "df_fold = pd.read_csv(os.path.join(PATH.DATA, \"fold.csv\"))\n",
        "df_train = df_train.merge(df_fold, on=\"id\", how=\"inner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZTltoxx9bqe",
        "outputId": "37922f4e-8c66-4566-efb0-ec3b34b9744b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-16, 11:26:27 - 14535\n",
            "2024-12-16, 11:26:27 - 4845\n"
          ]
        }
      ],
      "source": [
        "# ds = Dataset.from_csv(\"train.csv\")\n",
        "train_ds = Dataset.from_pandas(df_train.query(\"fold > 5\").query(\"fold <= 20\"))\n",
        "# eval_ds = Dataset.from_pandas(df_train.query(\"fold == 0\"))\n",
        "# train_ds = Dataset.from_pandas(df_train.query(\"fold >= 5\")).select(range(10))\n",
        "eval_ds = Dataset.from_pandas(df_train.query(\"fold < 5\"))\n",
        "\n",
        "print(len(train_ds))\n",
        "print(len(eval_ds))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-4KHQw-f9hHe"
      },
      "outputs": [],
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: dict) -> dict:\n",
        "        # Ensure that the keys exist in the batch before processing\n",
        "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch.get(\"prompt\", [])]\n",
        "        # metadata\n",
        "        # metadata_1 = [\"\\n\\n<note>: response_a is generated by \" + self.process_text(t) for t in batch.get(\"model_a\", [])]\n",
        "        # metadata_2 = [\"<note>: response_b is generated by \" + self.process_text(t) for t in batch.get(\"model_b\", [])]\n",
        "        # metadata_3 = [\"<note>: the language is \" + self.process_text(t) for t in batch.get(\"language\", [])]\n",
        "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch.get(\"response_a\", [])]\n",
        "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch.get(\"response_b\", [])]\n",
        "\n",
        "        # Concatenate all parts into one text field for tokenization\n",
        "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
        "\n",
        "        # Tokenize the texts\n",
        "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding=True)\n",
        "\n",
        "        # Handle the winner labels (mapping winner from 'model_a' to 0, 'model_b' to 1)\n",
        "        labels = []\n",
        "        winners = batch.get(\"winner\", [])\n",
        "\n",
        "        for winner in winners:\n",
        "            if winner == 'model_a':\n",
        "                label = 0\n",
        "            elif winner == 'model_b':\n",
        "                label = 1\n",
        "            # If the winner is neither 'model_a' nor 'model_b', you could choose to skip or handle the error here\n",
        "            else:\n",
        "                continue  # Or use `label = None` if you want to handle such cases separately\n",
        "            labels.append(label)\n",
        "\n",
        "        # Return tokenized output with labels\n",
        "        return {**tokenized, \"labels\": labels}\n",
        "\n",
        "    @staticmethod\n",
        "    def process_text(text: str) -> str:\n",
        "        if text is None:\n",
        "            return \"\"\n",
        "        return text.replace(\"null\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3dbb5dce687a4c5ebfdca410f2bca10b",
            "639ff58ed4734710a4d0e5c4b4341bfc",
            "6bbec1591ace422dac8ddd635f5a5cb5",
            "555617bd00fc43beb8c7e95dcc1e1c7b",
            "b6b839bb6ac8478cb55f8b04248b930b",
            "f4b5368698da4e4bb3247b31cf5b95de",
            "9170dc06e1f74285823b425820b58d38",
            "08ccddadeb93486291e4cc09fe4cdcf4",
            "258e952f3387480d927be39b1aa07317",
            "e5c633802d1541048407d56866a5d0c1",
            "cce585185a034e83b7266212202c4195",
            "bba479187a504be1bf11b5e137c2c9d4",
            "367e0c9ab2f34771b193bc52f8410289",
            "586784e9828242dca3e6d8ffe5a7cc67",
            "d2f9186dda3149aa901eaf0055cfbd48",
            "f45f4ae7a7524e2abe9ee2527b31cc72",
            "e1418a22b0ad48428aafa6826a542820",
            "b73422b9f3df451191b74abacdc16281",
            "5982d1f3a08a4a0b88bdaca1ba83fcda",
            "a13414ee3390413d9992db45d192ef97",
            "7eb83ab14cee467cb650675108ab5fb6",
            "ee39c3f38e29426da20480b7c3749a28"
          ]
        },
        "id": "fbu5oSzG9jYY",
        "outputId": "10acc240-8c2b-4d04-f65b-4619a1964a94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14535 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dbb5dce687a4c5ebfdca410f2bca10b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4845 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bba479187a504be1bf11b5e137c2c9d4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
        "train_ds = train_ds.map(encode, batched=True)\n",
        "eval_ds = eval_ds.map(encode, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "R5BBSTYPAw-9"
      },
      "outputs": [],
      "source": [
        "# train_ds = ds.filter(lambda x: x[\"fold\"] != 0)\n",
        "# eval_ds = ds.filter(lambda x: x[\"fold\"] == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pLNiSTzfEH0B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ1lhBpeEI1t"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5Yq3OVEuEX8p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction) -> dict:\n",
        "    # print(eval_pred.predictions)\n",
        "    # Extract predictions and labels from the EvalPrediction object\n",
        "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
        "    # print(logits)\n",
        "    # print(\"logits\", logits[0].shape)\n",
        "    # print(\"logits\", logits[1].shape)\n",
        "    # print(\"labels\", labels.shape)\n",
        "    # Convert logits to predicted labels (assuming binary classification with logits)\n",
        "    pred_labels = logits.argmax(axis=-1)  # For multi-class, use argmax along the correct axis\n",
        "    # print(pred_labels.shape)\n",
        "    # print(labels)\n",
        "    # Calculate accuracy and other metrics\n",
        "    accuracy = accuracy_score(labels, pred_labels)\n",
        "\n",
        "    # Return the metrics as a dictionary\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        # \"precision\": precision,\n",
        "        # \"recall\": recall,\n",
        "        # \"f1\": f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z2FSWe5vBCw9"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=PATH.MODELS,\n",
        "    overwrite_output_dir=True,\n",
        "    report_to=\"tensorboard\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.05,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=0.05,\n",
        "    optim=config.optim_type,\n",
        "    fp16=True,\n",
        "    learning_rate=config.lr,\n",
        "    warmup_steps=config.warmup_steps,\n",
        "    logging_dir=PATH.LOGS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H7e7dDL8I7LO"
      },
      "outputs": [],
      "source": [
        "class EvaluateAtEpochEndCallback(TrainerCallback):\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        control.should_evaluate = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HAq6CERWbTfc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "huIHYm6p-62O",
        "outputId": "4afb57c2-7343-4808-c2b8-692c1152bc07"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='1212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  42/1212 09:02 < 4:24:21, 0.07 it/s, Epoch 0.07/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='299' max='1212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 299/1212 2:01:22 < 6:13:06, 0.04 it/s, Epoch 0.49/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.939000</td>\n",
              "      <td>1.022747</td>\n",
              "      <td>0.502580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.788900</td>\n",
              "      <td>0.826215</td>\n",
              "      <td>0.503406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.646000</td>\n",
              "      <td>0.750143</td>\n",
              "      <td>0.504231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.748300</td>\n",
              "      <td>0.729250</td>\n",
              "      <td>0.499071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    callbacks=[EvaluateAtEpochEndCallback],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89l2aDXopK1G"
      },
      "outputs": [],
      "source": [
        "#  [ 525/5000 07:29 < 1:04:06, 1.16 it/s, Epoch 0.21/2]\n",
        "# Step\tTraining Loss\tValidation Loss\tAccuracy\n",
        "# 250\t0.630700\t0.735213\t0.497500\n",
        "# 500\t0.769500\t0.734566\t0.512500"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1BKPVfIgfde0bLPID54vHyyG3IYrlQYgR",
      "authorship_tag": "ABX9TyN5W/N7XRnA4r0yV9TY91yc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3dbb5dce687a4c5ebfdca410f2bca10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_639ff58ed4734710a4d0e5c4b4341bfc",
              "IPY_MODEL_6bbec1591ace422dac8ddd635f5a5cb5",
              "IPY_MODEL_555617bd00fc43beb8c7e95dcc1e1c7b"
            ],
            "layout": "IPY_MODEL_b6b839bb6ac8478cb55f8b04248b930b"
          }
        },
        "639ff58ed4734710a4d0e5c4b4341bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4b5368698da4e4bb3247b31cf5b95de",
            "placeholder": "​",
            "style": "IPY_MODEL_9170dc06e1f74285823b425820b58d38",
            "value": "Map: 100%"
          }
        },
        "6bbec1591ace422dac8ddd635f5a5cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ccddadeb93486291e4cc09fe4cdcf4",
            "max": 14535,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_258e952f3387480d927be39b1aa07317",
            "value": 14535
          }
        },
        "555617bd00fc43beb8c7e95dcc1e1c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5c633802d1541048407d56866a5d0c1",
            "placeholder": "​",
            "style": "IPY_MODEL_cce585185a034e83b7266212202c4195",
            "value": " 14535/14535 [00:13&lt;00:00, 1120.70 examples/s]"
          }
        },
        "b6b839bb6ac8478cb55f8b04248b930b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b5368698da4e4bb3247b31cf5b95de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9170dc06e1f74285823b425820b58d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ccddadeb93486291e4cc09fe4cdcf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "258e952f3387480d927be39b1aa07317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5c633802d1541048407d56866a5d0c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce585185a034e83b7266212202c4195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bba479187a504be1bf11b5e137c2c9d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_367e0c9ab2f34771b193bc52f8410289",
              "IPY_MODEL_586784e9828242dca3e6d8ffe5a7cc67",
              "IPY_MODEL_d2f9186dda3149aa901eaf0055cfbd48"
            ],
            "layout": "IPY_MODEL_f45f4ae7a7524e2abe9ee2527b31cc72"
          }
        },
        "367e0c9ab2f34771b193bc52f8410289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1418a22b0ad48428aafa6826a542820",
            "placeholder": "​",
            "style": "IPY_MODEL_b73422b9f3df451191b74abacdc16281",
            "value": "Map: 100%"
          }
        },
        "586784e9828242dca3e6d8ffe5a7cc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5982d1f3a08a4a0b88bdaca1ba83fcda",
            "max": 4845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a13414ee3390413d9992db45d192ef97",
            "value": 4845
          }
        },
        "d2f9186dda3149aa901eaf0055cfbd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eb83ab14cee467cb650675108ab5fb6",
            "placeholder": "​",
            "style": "IPY_MODEL_ee39c3f38e29426da20480b7c3749a28",
            "value": " 4845/4845 [00:04&lt;00:00, 1117.65 examples/s]"
          }
        },
        "f45f4ae7a7524e2abe9ee2527b31cc72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1418a22b0ad48428aafa6826a542820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b73422b9f3df451191b74abacdc16281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5982d1f3a08a4a0b88bdaca1ba83fcda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a13414ee3390413d9992db45d192ef97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eb83ab14cee467cb650675108ab5fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee39c3f38e29426da20480b7c3749a28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}